# src/utils/llm_callback.py
import time, json, logging
from typing import Any, Dict, Optional
from langchain.callbacks.base import BaseCallbackHandler
from langchain.schema import LLMResult

logger = logging.getLogger(__name__)

class DesignerNodeCallback(BaseCallbackHandler):
    """ä¸“é—¨ç›‘æ§ designer_node é‡Œ LLM çš„è€—æ—¶ã€è¾“å…¥ã€è¾“å‡º"""
    def __init__(self, node_name: str = "designer_node"):
        super().__init__()
        self.node_name = node_name

    # ---- å¼‚æ­¥ç‰ˆæœ¬ï¼ˆLangGraph é»˜è®¤èµ° asyncï¼‰ ----
    async def on_llm_start(
        self,
        serialized: Dict[str, Any],
        prompts: list[str],
        *,
        run_id,
        parent_run_id: Optional[str] = None,
        **kwargs: Any,
    ) -> None:
        self._start = time.time()
        # æŠŠå®Œæ•´è¯·æ±‚ä½“è½ç›˜ï¼ˆOpenAI æ¥å£èƒ½çœ‹åˆ° messages å­—æ®µï¼‰
        logger.info("[designer] ğŸ“¤ è¯·æ±‚ä½“: %s", json.dumps(kwargs.get("invocation_params", {}), ensure_ascii=False, indent=2))
        for idx, p in enumerate(prompts):
            logger.info("[designer] ğŸ“ prompt-%d: %.800s", idx, p)
            logger.info(f"[{self.node_name}] â±  LLM å¯åŠ¨, run_id={run_id}")
            # æŠŠ prompt ä¹Ÿè½ç›˜ï¼ˆå¤ªé•¿å¯ä»¥åªæ‰“å°å‰ 500 å­—ç¬¦ï¼‰
            for idx, p in enumerate(prompts):
                logger.info(f"[{self.node_name}] ğŸ“ prompt-{idx}: {p[:500]}...")

    async def on_llm_end(
        self, response: LLMResult, *, run_id, **kwargs: Any
    ) -> None:
        cost = time.time() - self._start
        logger.info(f"[{self.node_name}] âœ… LLM å®Œæˆ, run_id={run_id}, è€—æ—¶={cost:.2f}s")
        for g in response.generations:
            for chunk in g:
                logger.info(f"[{self.node_name}] ğŸ’¬ è¾“å‡º: {chunk.text[:500]}...")

    # ---- åŒæ­¥ç‰ˆæœ¬ï¼ˆé˜²æ­¢æœ‰äººæ‰‹åŠ¨ invokeï¼‰ ----
    def on_llm_start(self, *args, **kwargs):
        pass   # å¤ç”¨å¼‚æ­¥å³å¯ï¼Œè¿™é‡Œç•™ç©º
    def on_llm_end(self, *args, **kwargs):
        pass